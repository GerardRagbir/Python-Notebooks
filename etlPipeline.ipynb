{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzYJmIr2ysN1IDJBuAi0mx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardRagbir/Python-Notebooks/blob/main/etlPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL Pipeline"
      ],
      "metadata": {
        "id": "4uWop-Mr-KGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook goes through an arbitrary ETL workflow, and intends to act as a template of sorts for my pipeline process. It's not directly executable since it does require some setup, in fact I'll outline these now:\n",
        "\n",
        "\n",
        "*   **Database Connection**: in my code, I have configured the essentials for PostgreSQl via SQLAlchemy and PyODBC. This can be setup for any other supported database, or alternatively substituted for another package.\n",
        "*   **Environment Configuration**: it's always good practice to obfuscate/secure your sensitive credentials - especially in the event that a breach occurs and the source code is visibly available - it can compromise your entire database! Check out [this fantastic article from Github](https://github.blog/2022-01-27-beyond-sql-injection-owasp-tips-secure-database-access/).\n"
      ],
      "metadata": {
        "id": "FIqUa2bbTucM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "SXgam-Y9rjWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The codeblock below isn't necessary for an already configured environment - it just ensures the Jupyter notebook is configured properly as we intend to use 3.11 (and possibly newer provided newer versions do not present breaking changes).\n",
        "\n",
        "Our environnment will require the following:\n",
        "\n",
        "*   Python 3.11\n",
        "*   Packages: [SqlAlchemy](https://docs.sqlalchemy.org/en/14/), [PyODBC](https://github.com/mkleehammer/pyodbc/wiki), [Pandas](https://pandas.pydata.org/docs/)"
      ],
      "metadata": {
        "id": "oOT8LVjnVto9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Install python 3.11'''\n",
        "!sudo apt-get update -y # update system dependencies\n",
        "!sudo apt-get install python3.11 # update python to 3.11 (latest as of 2022-Oct)\n",
        "\n",
        "'''Install required packages'''\n",
        "!pip install sqlalchemy pyodbc pandas # required packages, modify as needed"
      ],
      "metadata": {
        "id": "aEOfvouC-WQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically you'd need to set the ENV VARIABLES to the PATH using specific methods via BASH for the OS you're on. However, since we're only concerned with Python 3.X, we'll use the following script:"
      ],
      "metadata": {
        "id": "j-F7fZA7oUi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write ENV variables to OS via a Python script such as:\n",
        "\n",
        "os.environ['PGPASS'] = 'password'\n",
        "os.environ['PGUID'] = 'etluser'\n",
        "\n",
        "\n",
        "'''Alternative for quickly setting env variables'''\n",
        "\n",
        "# %env PGPASS=password\n",
        "# %env PGUID=etluser"
      ],
      "metadata": {
        "id": "rRugWj3bpxUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Modules\n",
        "\n",
        "<b>Resources</b>\n",
        "\n",
        "SQL Alchemy: https://towardsdatascience.com/sqlalchemy-python-tutorial-79a577141a91\n",
        "\n",
        "PyODBC: https://learn.microsoft.com/en-us/sql/connect/python/pyodbc/python-sql-driver-pyodbc?view=sql-server-ver16\n",
        "\n",
        "Pandas: https://www.w3schools.com/python/pandas/default.asp"
      ],
      "metadata": {
        "id": "ZqqpzM-Bpx1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQDhCMiY-JJD"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "import pyodbc as odbc\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get variables from OS Environment\n",
        "\n",
        "REMINDER: Never store variables within source!\n",
        "\n",
        "The codeblock below assumes you've stored your database credentials (if you're using a username and password, some alternative verification methods may be present so consult the documentation and research best practices for your solution!)"
      ],
      "metadata": {
        "id": "KneCa5G9-woB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "These linked variables can be refactored directly to their methods, \n",
        "I am just separating them for explanation during live sessions!\n",
        "\n",
        "eg pwd = os.environp['PGPASS'] would give a similar result.\n",
        "'''\n",
        "\n",
        "ETLPWD = str('PGPASS')\n",
        "ETLUID = str('PGUID')\n",
        "\n",
        "pwd = os.environ[ETLPWD]\n",
        "uid = os.environ[ETLUID]"
      ],
      "metadata": {
        "id": "aHCiQtPs-9cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''SQL Connection'''\n",
        "driver = \"{ODBC Driver 17 for SQL Server}\" #eg ODBC Driver 17 for SQL Server\n",
        "server = \"localhost\" #use localhost if using a local machine\n",
        "database = \"#Name_of_DB_HERE\" \n",
        "port = 5432\n",
        "\n",
        "table = \"#tableName\"\n",
        "\n",
        "\n",
        "CONNECTION_PATH = f'DRIVER={driver};SERVER={server}\\SQLEXPRESS;DATABASE={database};UID={uid};PWD={pwd}'"
      ],
      "metadata": {
        "id": "P-gsm_9aCJrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to: https://www.connectionstrings.com/formating-rules-for-connection-strings/"
      ],
      "metadata": {
        "id": "Z05_vdWPjZtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"\"\"\n",
        "        SELECT t.name AS table_name\n",
        "        FROM sys.tables t WHERE t.name IN ('DimProduct', 'DimProductSubcategory', 'DimProductCategory', 'DimSalesTerritory', 'FactInternetSales');\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "g8gW-w8sYHz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you're unaware of what ETL is, it stands for Extract, Transform, Load - and is meant to encompass a pipeline or methodology for automating the populating of a DWH (Data Warehouse)."
      ],
      "metadata": {
        "id": "ITbLMCM_cVea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It begins by EXTRACTING data from the existing API/s, or from the database/s directly. This source data is referred to as the OLTP (Online Transactional Processing) which is meant for transactional-systems, mainly APIs for your backend and main client applications. This data intends to feed your OLAP (Online Analytical Processing), which is ideally a read-optimized (probably columnar) database known as a Data Warehouse.\n",
        "\n",
        "NOTE: we could just as easily use a standard set of tools such as Apache Kafka, Apache Nifi, AWS Kinesis or Hevo. These ingest solutions are usually preferred for massive datasets, versus the basic python code below.\n",
        "\n",
        "If we were using a memory limited host system for the extract script below, it may become necessary to implement some form of bathc processing. Luckily for us, Pandas [provides us with some ideas](https://pandas.pydata.org/docs/user_guide/scale.html), one of which is [chunking](https://towardsai.net/p/data-science/efficient-pandas-using-chunksize-for-large-data-sets-c66bf3037f93)"
      ],
      "metadata": {
        "id": "2Q7cNOTocvda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract():\n",
        "  try:\n",
        "    src_connect = odbc.connect(CONNECTION_PATH)\n",
        "    cursor = src_connect.cursor()\n",
        "\n",
        "    #execute query\n",
        "    cursor.execute(QUERY)\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    for row in rows:\n",
        "      df = pd.read_sql_query(f'SELECT * FROM {table[0]}', src_connect)\n",
        "      load(df, table[0])\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Extraction Error: {e}\")\n",
        "\n",
        "  finally:\n",
        "    src_connect.close()"
      ],
      "metadata": {
        "id": "SA_EaPbnC0Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "  try:\n",
        "    rows_imported = 0\n",
        "    engine = create_engine(f'postgresql://{uid}:{pwd}@{server}:{port}/{table}')\n",
        "    print(f'Importing rows: {rows_imported} to {rows_imported+len(df)} ... for table {table}')\n",
        "\n",
        "    #commit dataframe to database (postgresql here)\n",
        "    df.to_sql(f'stg_{table}', engine, if_exists='replace', index=False)\n",
        "    rows_imported += len(df)\n",
        "    print(\"Data imported successfully!\")\n",
        "  except Exception as e:\n",
        "    print(f\"Load Error: {e}\")\n"
      ],
      "metadata": {
        "id": "H2BEjisdFpar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run ETL"
      ],
      "metadata": {
        "id": "Tb6knZhwmhiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  extract()\n",
        "except Exception as e:\n",
        "  print(f\"Error during ETL: {e}\")"
      ],
      "metadata": {
        "id": "kqU1SFGpmfI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}